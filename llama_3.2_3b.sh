#!/bin/bash


OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 256             --stddev-input-tokens 6             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 10             --timeout 1000000            --num-concurrent-requests 1             --results-dir llama_perf_llama_3b/256_2048_1             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_1.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 256             --stddev-input-tokens 6             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 500             --timeout 1000000            --num-concurrent-requests 50             --results-dir llama_perf_llama_3b/256_2048_50             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_2.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 256             --stddev-input-tokens 6             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 640             --timeout 1000000            --num-concurrent-requests 64             --results-dir llama_perf_llama_3b/256_2048_64             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_3.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 256             --stddev-input-tokens 6             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 1000             --timeout 1000000            --num-concurrent-requests 100             --results-dir llama_perf_llama_3b/256_2048_100             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_4.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 256             --stddev-input-tokens 6             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 1280             --timeout 1000000            --num-concurrent-requests 128             --results-dir llama_perf_llama_3b/256_2048_128             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_5.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 256             --stddev-input-tokens 6             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 1600             --timeout 1000000            --num-concurrent-requests 160             --results-dir llama_perf_llama_3b/256_2048_160             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_6.log




OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 512             --stddev-input-tokens 12             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 10             --timeout 1000000            --num-concurrent-requests 1             --results-dir llama_perf_llama_3b/512_2048_1             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_7.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 512             --stddev-input-tokens 12             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 500             --timeout 1000000            --num-concurrent-requests 50             --results-dir llama_perf_llama_3b/512_2048_50             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_8.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 512             --stddev-input-tokens 12             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 640             --timeout 1000000            --num-concurrent-requests 64             --results-dir llama_perf_llama_3b/512_2048_64             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_9.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 512             --stddev-input-tokens 12             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 1000             --timeout 1000000            --num-concurrent-requests 100             --results-dir llama_perf_llama_3b/512_2048_100             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_10.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 512             --stddev-input-tokens 12             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 1280             --timeout 1000000            --num-concurrent-requests 128             --results-dir llama_perf_llama_3b/512_2048_128             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_11.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 512             --stddev-input-tokens 12             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 1600             --timeout 1000000            --num-concurrent-requests 160             --results-dir llama_perf_llama_3b/512_2048_160             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_1_12.log





OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 2048             --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 10             --timeout 1000000            --num-concurrent-requests 1             --results-dir llama_perf_llama_3b/2048_256_1             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_1.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 2048             --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 500             --timeout 1000000            --num-concurrent-requests 50             --results-dir llama_perf_llama_3b/2048_256_50             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_2.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 2048             --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 640             --timeout 1000000            --num-concurrent-requests 64             --results-dir llama_perf_llama_3b/2048_256_64             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_3.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 2048             --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 1000             --timeout 1000000            --num-concurrent-requests 100             --results-dir llama_perf_llama_3b/2048_256_100             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_4.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 2048             --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 1280             --timeout 10000000            --num-concurrent-requests 128             --results-dir llama_perf_llama_3b/2048_256_128             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_5.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 2048             --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 1600             --timeout 10000000            --num-concurrent-requests 160             --results-dir llama_perf_llama_3b/2048_256_160             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_6.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 2048             --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 2560             --timeout 10000000            --num-concurrent-requests 256             --results-dir llama_perf_llama_3b/2048_256_256             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_7.log




OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 4096            --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 10             --timeout 10000000            --num-concurrent-requests 1             --results-dir llama_perf_llama_3b/4096_256_1             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_8.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 4096            --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 500             --timeout 10000000            --num-concurrent-requests 50             --results-dir llama_perf_llama_3b/4096_256_50             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_9.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 4096            --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 640             --timeout 10000000            --num-concurrent-requests 64             --results-dir llama_perf_llama_3b/4096_256_64             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_10.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 4096            --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 1000             --timeout 10000000            --num-concurrent-requests 100             --results-dir llama_perf_llama_3b/4096_256_100             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_11.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 4096            --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 1280             --timeout 10000000            --num-concurrent-requests 128             --results-dir llama_perf_llama_3b/4096_256_128             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_12.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 4096            --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 1600             --timeout 10000000            --num-concurrent-requests 160             --results-dir llama_perf_llama_3b/4096_256_160             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_13.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 4096            --stddev-input-tokens 102             --mean-output-tokens 256             --stddev-output-tokens 6             --max-num-completed-requests 2560             --timeout 1000000            --num-concurrent-requests 256             --results-dir llama_perf_llama_3b/4096_256_256             --llm-api openai             --additional-sampling-params '{"max_tokens": "128", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_2_14.log





OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 2048             --stddev-input-tokens 102             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 10             --timeout 1000000            --num-concurrent-requests 1             --results-dir llama_perf_llama_3b/2048_2048_1             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_3_1.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 2048             --stddev-input-tokens 102             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 500             --timeout 1000000            --num-concurrent-requests 50             --results-dir llama_perf_llama_3b/2048_2048_50             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_3_2.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 2048             --stddev-input-tokens 102             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 640             --timeout 1000000            --num-concurrent-requests 64             --results-dir llama_perf_llama_3b/2048_2048_64             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_3_3.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 2048             --stddev-input-tokens 102             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 1000             --timeout 1000000            --num-concurrent-requests 100             --results-dir llama_perf_llama_3b/2048_2048_100             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_3_4.log



OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 4096             --stddev-input-tokens 102             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 10             --timeout 1000000            --num-concurrent-requests 1             --results-dir llama_perf_llama_3b/4096_2048_1             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_3_5.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 4096             --stddev-input-tokens 102             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 500             --timeout 1000000            --num-concurrent-requests 50             --results-dir llama_perf_llama_3b/4096_2048_50             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_3_6.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 4096             --stddev-input-tokens 102             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 640             --timeout 1000000            --num-concurrent-requests 64             --results-dir llama_perf_llama_3b/4096_2048_64             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_3_7.log

OPENAI_API_BASE='http://localhost:4000/v1' OPENAI_API_KEY='1234' python llmperf/token_benchmark_ray.py             --model meta-llama/Llama-3.2-3B-Instruct          --mean-input-tokens 4096             --stddev-input-tokens 102             --mean-output-tokens 2048             --stddev-output-tokens 102             --max-num-completed-requests 1000             --timeout 1000000            --num-concurrent-requests 100             --results-dir llama_perf_llama_3b/4096_2048_100             --llm-api openai             --additional-sampling-params '{"max_tokens": "2048", "ignore_eos": "True"}' 2>&1 | tee -a llama_perf_llama_3b/llama_3_3b_I_3_8.log




#chmod +x script.sh
#./script.sh
